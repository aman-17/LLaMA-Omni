{
  "stage": 2,
  "stage1_model_path": "./outputs/stage1/final_model",
  "kmeans_model_path": "/path/to/mhubert_base_vp_en_es_fr_it3_L11_km1000.bin",
  "validation_data_path": null,
  "resume_from_checkpoint": null,
  "model_name_or_path": "meta-llama/Llama-3.1-8B-Instruct",
  "version": "v0",
  "freeze_backbone": true,
  "tune_speech_projector": false,
  "tune_speech_encoder": false,
  "tune_speech_generator_only": true,
  "speech_encoder_type": "whisper",
  "speech_encoder": "openai/whisper-large-v3",
  "pretrain_speech_projector": null,
  "speech_projector_type": "linear",
  "speech_generator_type": "ctc",
  "ctc_decoder_config": "(2,4096,32,11008)",
  "ctc_upsample_factor": 25,
  "ctc_loss_weight": 1.0,
  "unit_vocab_size": 1000,
  "speech_encoder_ds_rate": 5,
  "speech_encoder_hidden_size": 1280,
  "data_path": "/path/to/InstructS2S-200K",
  "is_multimodal": true,
  "input_type": "mel",
  "speech_normalize": false,
  "mel_size": 128,
  "has_tgt_units": true,
  "output_dir": "./outputs/stage2",
  "num_train_epochs": 3,
  "per_device_train_batch_size": 32,
  "per_device_eval_batch_size": 32,
  "gradient_accumulation_steps": 1,
  "evaluation_strategy": "epoch",
  "save_strategy": "epoch",
  "save_steps": 1,
  "save_total_limit": 3,
  "learning_rate": 2e-4,
  "weight_decay": 0.0,
  "warmup_ratio": 0.03,
  "lr_scheduler_type": "cosine",
  "logging_steps": 10,
  "remove_unused_columns": false,
  "dataloader_drop_last": false,
  "dataloader_num_workers": 4,
  "dataloader_pin_memory": true,
  "max_grad_norm": 1.0,
  "speech_projector_lr": null,
  "report_to": "wandb",
  "run_name": "llama-omni-stage2"
}